# Install required packages
!pip install datasets transformers evaluate -q

# Import necessary libraries
from datasets import load_dataset
import pandas as pd
from transformers import GPT2TokenizerFast, GPT2ForSequenceClassification
from transformers import TrainingArguments, Trainer
import evaluate
import numpy as np

# Load the dataset
dataset = load_dataset('mteb/tweet_sentiment_extraction')

# Convert to DataFrame (optional step to explore)
df = pd.DataFrame(dataset['train'])
print(df.head())

# Initialize the GPT-2 tokenizer
tokenizer = GPT2TokenizerFast.from_pretrained("gpt2")
tokenizer.pad_token = tokenizer.eos_token  # GPT-2 doesn't have a pad token by default

# Tokenization function
def tokenize_function(examples):
    return tokenizer(examples["text"], padding="max_length", truncation=True)

# Tokenize the full dataset
tokenized_datasets = dataset.map(tokenize_function, batched=True)

# Sample a smaller subset for faster training/testing
small_train_dataset = tokenized_datasets["train"].shuffle(seed=42).select(range(1000))
small_eval_dataset = tokenized_datasets["test"].shuffle(seed=42).select(range(1000))

# Load GPT-2 model for sequence classification
model = GPT2ForSequenceClassification.from_pretrained("gpt2", num_labels=3)

# Load evaluation metric
accuracy = evaluate.load("accuracy")

# Define evaluation metric function
def compute_metrics(eval_pred):
    logits, labels = eval_pred
    predictions = np.argmax(logits, axis=-1)
    return accuracy.compute(predictions=predictions, references=labels)

# Preprocessing (re-tokenizing after sampling)
def preprocess_function(examples):
    return tokenizer(examples["text"], truncation=True)

small_train_dataset = small_train_dataset.map(preprocess_function, batched=True)
small_eval_dataset = small_eval_dataset.map(preprocess_function, batched=True)

# Define training arguments
training_args = TrainingArguments(
    output_dir="test_trainer",
    per_device_train_batch_size=1,
    per_device_eval_batch_size=1,
    gradient_accumulation_steps=4,
    evaluation_strategy="epoch"
)

# Initialize Trainer
trainer = Trainer(
    model=model,
    args=training_args,
    train_dataset=small_train_dataset,
    eval_dataset=small_eval_dataset,
    compute_metrics=compute_metrics,
)

# Train the model
trainer.train()

# Evaluate the model
eval_results = trainer.evaluate()
print("Evaluation results:", eval_results)
